<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="EEVR: A Virtual Reality-Based Emotion Dataset Featuring Paired Physiological Signals and Textual Descriptions.">
  <meta name="keywords" content="EEVR">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EEVR: VR Based Emotion Dataset</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation"></nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">EEVR: A Dataset of Paired Physiological Signals and
              Textual Descriptions for Joint Emotion Representation
              Learning</h1>
            <div class="is-size-5 publication-authors">
              <a href="https://alchemy18.github.io/pragyasingh/">
                <span class="author-block">
                  Pragya Singh<sup>1</sup>,
                </span>
              </a>

              <a href="https://www.linkedin.com/in/ritvik-budhiraja/">
                <span class="author-block">
                  Ritvik Budhiraja<sup>1</sup>,
                </span>
              </a>
              <a
                href="https://www.linkedin.com/in/ankush-gupta-9a9211224?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app">
                <span class="author-block">
                  Ankush Gupta<sup>1</sup>,
                </span>
              </a>
              <a href="https://www.linkedin.com/in/anshul-goswami-83844b211/">
                <span class="author-block">
                  Anshul Goswami<sup>1</sup>,
                </span>
              </a>
              <a href="https://www.rit.edu/computing/directory/mjkvcs-mohan-kumar">
                <span class="author-block">
                  Mohan Kumar<sup>2</sup>, and
                </span>
              </a>
              <a href="https://www.pushpendrasingh.info/">
                <span class="author-block">
                  Pushpendra Singh<sup>1</sup>
                </span>
              </a>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block author-titles"><sup>1</sup> IIIT-D, New Delhi, India</span><br>
              <span class="author-block author-titles"><sup>2</sup> RIT, Rochester, New York, USA</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://nips.cc/virtual/2024/poster/97493"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>NeurIPS Event Page</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-arrow-circle-down"></i>
                    </span>
                  </a> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/teaser.png" alt="">
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">EEVR</span> (Emotion Elicitation in Virtual Reality) is a novel dataset for language
          supervision-based pre-training and emotion recognition tasks
        </h2>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed
              for language supervision-based pre-training of emotion recognition tasks,
              such as valence and arousal classification. It features high-quality physiological
              signals, including <b>electrodermal activity (EDA)</b> and <b>photoplethysmography (PPG)</b>,
              acquired through emotion elicitation via 360-degree virtual reality (VR) videos.
              Additionally, it includes subject-wise textual descriptions of emotions experienced
              during each stimulus gathered from qualitative interviews.
              Emotional stimuli are selected to induce diverse emotions across
              all four quadrants of <a
                href="https://www.researchgate.net/publication/232501584_Affect_Grid_A_Single-Item_Scale_of_Pleasure_and_Arousal">Russell's
                circumplex model</a>.
            </p>
            <p>
              The dataset consists
              of recordings from <b>37 participants</b> and is the first dataset to pair raw text with
              physiological signals, providing additional contextual information that objective
              labels cannot offer. To leverage this dataset, we introduced the <b>Contrastive Language
                Signal Pre-training (CLSP)</b> method, which jointly learns representations
              using pairs of physiological signals and textual descriptions.
            </p>
            <p>
              Our results show
              that integrating self-reported textual descriptions with physiological signals significantly
              improves performance on emotion recognition tasks, such as arousal
              and valence classification. Moreover, our pre-trained CLSP model demonstrates
              <b>strong zero-shot transferability</b> to existing datasets, outperforming supervised base
              line models, suggesting that the representations learned by our method are more
              contextualized and generalized. The dataset also includes baseline models for
              arousal, valence, and emotion classification, as well as code for data cleaning
              and feature extraction.
              <br><br>
              Kindly fill the following form to get access to the full dataset: <a
                href="https://forms.gle/ZsZLgaqBx9i829LL6">form link</a>.
              <br>
              Codes for EEVR can be found <a href="https://github.com/alchemy18/EEVR/">here</a>.
            </p>
            <p>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Contrastive Language
            Signal Pre-training (CLSP) method</h2>
          <div class="content has-text-justified">
            <img src="./static/images/CLSP_arch.png" alt="">
            <p class="image-caption has-text-centered">
              The Architecture for Contrastive-Language Singal Pre-Training (CLSP)
            </p>
            <p>
              To underscore the importance of integrating textual descriptions in emotion recognition, we introduce
              the Contrastive Language-Signal Pre-training (CLSP) method for extracting more contextualized
              representations.
            </p>
            <p>
              The model was trained on <i>physiological signals</i> and <i>text pairs</i> to learn a <b>joint
                embedding space</b>, where both modalities are closely aligned using a <a
                href="https://proceedings.mlr.press/v139/radford21a">contrastive loss
                function</a>.
              Following pre-training, we evaluated the model's performance on test subject data
              using the <b>leave-one-subject-out</b> cross-validation approach.
              <br>
              CLSP uses separate neural networks to process two types of input data: physiological signals (PPG and EDA)
              and text. For signal data, it uses linear, hidden layers sizes of 50 and 100, while for text, it
              applies a pre-trained DistilBERT model. The model then optimizes a contrastive objective to increase
              similarity in positive pairs and decrease it in negative ones. We observed that CLSP significantly
              improved emotion
              recognition in arousal and valence tasks compared to models trained only on signal data. This underscores
              the value of adding text-based supervision to enhance emotion representation from physiological signals.
            </p>
            <img src="./static/images/CLSP_results.png" alt="">
            <p class="image-caption has-text-centered">
              Results for Physiological Baseline without text using Hand-crafted features +NN and with text using
              CLSP on 296 text-signal pairs (seed=43, epoch=15)
            </p>
          </div>
        </div>
      </div>

      <!--/ Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">License</h2>
          <div class="content has-text-justified">
            <p>
              EEVR: A Virtual Reality-Based Emotion Dataset Featuring Paired Physiological Signals and Textual
              Descriptions Â© 2024 by Pragya Singh, Pushpendra Singh is licensed under Creative Commons
              Attribution-NonCommercial-ShareAlike 4.0 International. To view a copy of this license, visit
              <a
                href="https://creativecommons.org/licenses/by-nc-sa/4.0/">https://creativecommons.org/licenses/by-nc-sa/4.0/</a>.
            </p>
            <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><img
                style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"
                src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1" alt=""><img
                style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"
                src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1" alt=""><img
                style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"
                src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1" alt=""><img
                style="height:22px!important;margin-left:3px;vertical-align:text-bottom;"
                src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1" alt=""></a></p>
          </div>
        </div>
      </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Accepted at NeurIPS dataset track <br>@inproceedings{ singh2024eevr, title={{EEVR}: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning}, author={Pragya Singh and Ritvik Budhiraja and Ankush Gupta and Anshul Goswami and Mohan Kumar and Pushpendra Singh}, booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, year={2024}, url={https://openreview.net/forum?id=qgzdGyQcDt} }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              The template was taken from the following <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of the <a href="https://nerfies.github.io/">Nerfies website</a>. Any usage of the above
              template should link back to the
              Nerfies page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
